summary(lm1)
#此时进行多元线性回归，发现有数个变量有NA，这是因为在将分类变量处理成虚拟变量时，衍生的虚拟变量有多重共线性，例如，fueltype产生的两个虚拟变量diesel和gas对应元素相加为一，这种情况下，我们可以删除有多重共线性的虚拟变量，例如，drivewheel衍生的4wd，fwd，rwd三个虚拟变量，删去rwd，而4wd和fwd均为0时就是rwd。
library(MASS)
stud_resids <- studres(lm1)
plot(car_price_re$price, stud_resids, ylab=' Studentized Residuals ', xlab=' Displacement ')
car_price_renew1<-car_price_re[-17,]
lmnew1<-lm(price~.,car_price_renew1)
stud_residsnew1 <- studres(lmnew1)
plot(car_price_renew1$price, stud_residsnew1, ylab=' Studentized Residuals ', xlab=' Displacement ')
stud_residsnew1
car_price_re$gas<-NULL
car_price_re$turbo<-NULL
car_price_re$wagon<-NULL
car_price_re$rwd<-NULL
car_price_re$rear<-NULL
car_price_re$ohcv<-NULL
car_price_re$mfi<-NULL
##这两个虚拟变量出现NA的原因似乎不是因为产生虚拟变量时出现的多重共线性？##
car_price_re$rotor<-NULL
car_price_re$idi<-NULL
#抽取30%的数据作为检验组
set.seed(2021)
n<-round(nrow(car_price_re)*3/10)
uni<-sample(1:nrow (car_price_re),n,replace=T)
testGroup<-car_price_re[uni,]
car_price_re<-car_price_re[-uni,]
car_enginetype$enginetype[car_enginetype$enginetype=='ohc']<-0
car_enginetype$enginetype[car_enginetype$enginetype=='ohcv']<-1
car_enginetype$enginetype[car_enginetype$enginetype=='rotor']<-2
car_enginetype$enginetype[car_enginetype$enginetype=='dohc']<-2
car_enginetype$enginetype[car_enginetype$enginetype=='ohcf']<-3
car_enginetype$enginetype[car_enginetype$enginetype=='dohcv']<-4
car_fuelsystem$fuelsystem[car_fuelsystem$fuelsystem=='mpfi']<-0
car_fuelsystem$fuelsystem[car_fuelsystem$fuelsystem=='1bbl']<-1
car_fuelsystem$fuelsystem[car_fuelsystem$fuelsystem=='2bbl']<-2
car_fuelsystem$fuelsystem[car_fuelsystem$fuelsystem=='4bbl']<-3
car_fuelsystem$fuelsystem[car_fuelsystem$fuelsystem=='spfi']<-4
car_fuelsystem$fuelsystem[car_fuelsystem$fuelsystem=='idi']<-5
car_fuelsystem$fuelsystem[car_fuelsystem$fuelsystem=='spdi']<-6
car_fuelsystem$fuelsystem[car_fuelsystem$fuelsystem=='mfi']<-7
library(ggplot2)
x=car_price$curbweight
df<-data.frame(x=car_price$curbweight,y=car_price$price)
df1<-data.frame(x=car_price$horsepower,y=car_price$price)
df4<-data.frame(x=car_price$carlength,y=car_price$price)
ggplot(df,aes(x=x,y=y))+geom_point()+labs(title='relationship')
ggplot(df1,aes(x=x,y=y))+geom_point()+labs(title='relationship')
ggplot(df4,aes(x=x,y=y))+geom_point()+labs(title='relationship')
library(Hmisc)
df<-car_price_re[,c('curbweight','front','mpfi','carwidth','l','fwd','sedan','ohcf','horsepower','convertible','price')]
res2 <- rcorr(as.matrix(df))
res2$r
library(corrplot)
corrplot(res2$r, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
lm1<-lm(price~.,car_price_re)
lm1
plot(lm1)
#从残差图看出，误差项具有明显的异方差性
rstudent(lm1)
#|SRE|<3，说明不存在因变量的异常值
#计算库克距离
cook<-cooks.distance(lm1)
sorted_cook<-cook[order(cook,decreasing=TRUE)]
plot(sorted_cook,type='o',pch=16,xlab='observation',ylab="cook's distance")
abline(h=0.5,lty=2,col='red')
#cook distance<1，不认为有自变量异常点
library(MASS)
bc<-boxcox(price~.,data=car_price_re)
lambda <- bc$x[which.max(bc$y)]
lambda
car_price_re_bc<-car_price_re
car_price_re_bc$price<-(car_price_re$price^lambda-1)/lambda
#stepwise regression aentry=0.01 aremoval=0.02
add1(lm(price~1,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
add1(lm(price~curbweight,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
add1(lm(price~curbweight+horsepower,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
drop1(lm(price~curbweight+horsepower,data = car_price_re_bc),test="F")
add1(lm(price~curbweight+horsepower+hatchback,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
drop1(lm(price~curbweight+horsepower+hatchback,data = car_price_re_bc),test="F")
add1(lm(price~curbweight+horsepower+hatchback+mpfi,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
drop1(lm(price~curbweight+horsepower+hatchback+mpfi,data = car_price_re_bc),test="F")
add1(lm(price~curbweight+horsepower+hatchback+front+mpfi,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
drop1(lm(price~curbweight+horsepower+hatchback+front+mpfi,data = car_price_re_bc),test="F")
add1(lm(price~curbweight+horsepower+hatchback+front+mpfi+carwidth,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
drop1(lm(price~curbweight+horsepower+hatchback+front+mpfi+carwidth,data = car_price_re_bc),test="F")
lm2<-lm(price~curbweight+horsepower+hatchback+front+mpfi+carwidth,data = car_price_re_bc)
summary(lm2)
library(car)
influencePlot(lm2)
vif(lm2)
sqrt(vif(lm2))>10
#每个响应变量对应vif的平方小于10，说明多重共线性较小
add1(lm(price~1,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
add1(lm(price~wheelbase,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
add1(lm(price~wheelbase+carlength,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
add1(lm(price~wheelbase+carlength+curbweight,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
add1(lm(price~wheelbase+carlength+curbweight+horsepower,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
add1(lm(price~wheelbase+carlength+curbweight+horsepower+front,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
add1(lm(price~wheelbase+carlength+curbweight+horsepower+front+mpfi,data = car_price_re_bc), price ~ symboling+wheelbase+carlength+carwidth+carheight+curbweight+cylindernumber+enginesize+boreratio+stroke+compressionratio+horsepower+peakrpm+citympg+highwaympg+diesel+std+convertible+hardtop+hatchback+sedan+fwd+front+dohc+dohcv+l+ohc+ohcf+mpfi+spdi+spfi, test="F")
lm3<-lm(price~wheelbase+carlength+curbweight+horsepower+front+mpfi,data = car_price_re_bc)
lm3
library(car)
influencePlot(lm3)
vif(lm3)
sqrt(vif(lm3))>10
#共线性不严重 但是175号变量为关于y的异常值
#lasso回归
car_price$car_ID<-NULL
car_price_re$price<-(car_price_re$price^lambda-1)/lambda
library(glmnet)
x<-as.matrix(car_price_re[colnames(car_price_re)!="price"])
y<-as.matrix(car_price_re[colnames(car_price_re)=="price"])
fit<-glmnet(x,y,alpha=1)
#lasso筛选变量动态过程图
plot(fit,xvar="lambda",label=T)
#我们可以看到，当lambda越大，各估计参数相应的也被压缩得更小，而当lambda达到一定值以后，一部分不重要的变量将被压缩为0，代表该变量已被剔除出模型，图中从左至左右断下降的曲线如同被不断增大的lambda一步一步压缩，直到压缩为0。
#利用交叉检验挑选lambda，选择平均误差最小的lambda
model_cv<-cv.glmnet(x,y,alpha=1)
plot(model_cv)
best_lambda <- model_cv$lambda.min
best_lambda
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)
#如变量没有显示系数，即lasso回归收缩系数为零。这意味着它完全被排除在模型之外，因为它的影响力不够。系数非0的变量即为我们筛选的重要特征.
tidy_lasso <- broom::tidy(lasso_model)
tidy_lasso
#lasso回归检验R方
testGroup$car_ID<-NULL
x_new<-as.matrix(testGroup[colnames(testGroup)!="price"])
y_new_lm2<-(y_new^lambda-1)/lambda
y_new<-as.matrix(testGroup[colnames(testGroup)=="price"])
y_lasso_predict <- predict(lasso_model, s = best_lambda, newx = x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse <- sum((y_lasso_predict - y_new_lm2)^2)
rsq <- 1 - sse/sst
rsq
#逐步回归检验R方
x_new<-data.frame(x_new)
y_stepwise_predict<-predict.lm(lm2,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse2 <- sum((y_stepwise_predict - y_new_lm2)^2)
rsq2 <- 1 - sse2/sst
rsq2
#前进法检验R方
y_forward_predict<-predict.lm(lm3,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse3 <- sum((y_forward_predict - y_new_lm2)^2)
rsq3 <- 1 - sse3/sst
rsq3
y_lasso <- predict(lasso_model, s = best_lambda, newx = x)
sst <- sum((y - mean(y))^2)
sse <- sum((y_lasso - y)^2)
rsq <- 1 - sse/sst
rsq
#计算R-adjust
n<-nrow(testGroup)
k1<-15
k2<-lm2$rank-1
k3<-lm3$rank-1
radjust_lasso<-1-(1-rsq)*(n-1)/(n-k1-1)
radjust_lasso
radjust_stepwise<-1-(1-rsq2)*(n-1)/(n-k2-1)
radjust_stepwise
radjust_forward<-1-(1-rsq3)*(n-1)/(n-k3-1)
radjust_forward
#使用预测组计算MSE判断最优模型
n_new<-nrow(testGroup)
mse_lasso<-mean((y_new_lm2-y_lasso_predict)^2)
mse_stepwise<-mean((y_new_lm2-y_stepwise_predict)^2)
mse_forward<-mean((y_new_lm2-y_forward_predict)^2)
mse_lasso
mse_stepwise
mse_forward
y_new_lm2<-log(y_new_lm2*lambda+1,lambda)
y_new_lm2<-log(y_new_lm2*lambda+1,lambda)
#lasso回归检验R方
testGroup$car_ID<-NULL
x_new<-as.matrix(testGroup[colnames(testGroup)!="price"])
y_new<-as.matrix(testGroup[colnames(testGroup)=="price"])
y_new_lm2<-(y_new^lambda-1)/lambda
y_lasso_predict <- predict(lasso_model, s = best_lambda, newx = x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse <- sum((y_lasso_predict - y_new_lm2)^2)
rsq <- 1 - sse/sst
rsq
#逐步回归检验R方
x_new<-data.frame(x_new)
y_stepwise_predict<-predict.lm(lm2,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse2 <- sum((y_stepwise_predict - y_new_lm2)^2)
rsq2 <- 1 - sse2/sst
rsq2
#前进法检验R方
y_forward_predict<-predict.lm(lm3,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse3 <- sum((y_forward_predict - y_new_lm2)^2)
rsq3 <- 1 - sse3/sst
rsq3
y_lasso <- predict(lasso_model, s = best_lambda, newx = x)
sst <- sum((y - mean(y))^2)
sse <- sum((y_lasso - y)^2)
rsq <- 1 - sse/sst
rsq
#计算R-adjust
n<-nrow(testGroup)
k1<-15
k2<-lm2$rank-1
k3<-lm3$rank-1
radjust_lasso<-1-(1-rsq)*(n-1)/(n-k1-1)
radjust_lasso
radjust_stepwise<-1-(1-rsq2)*(n-1)/(n-k2-1)
radjust_stepwise
radjust_forward<-1-(1-rsq3)*(n-1)/(n-k3-1)
radjust_forward
y_new_lm2<-log(y_new_lm2*lambda+1,lambda)
y_lasso_predict<-log(y_lasso_predict*lambda+1,lambda)
y_lasso_predict<-log(y_lasso_predict*lambda+1,lambda)
y_stepwise_predict<-log(y_stepwise_predict*lambda+1,lambda)
y_forward_predict<-log(y_forward_predict*lambda+1,lambda)
#lasso回归检验R方
testGroup$car_ID<-NULL
x_new<-as.matrix(testGroup[colnames(testGroup)!="price"])
y_new<-as.matrix(testGroup[colnames(testGroup)=="price"])
y_new_lm2<-(y_new^lambda-1)/lambda
y_lasso_predict <- predict(lasso_model, s = best_lambda, newx = x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse <- sum((y_lasso_predict - y_new_lm2)^2)
rsq <- 1 - sse/sst
rsq
#逐步回归检验R方
x_new<-data.frame(x_new)
y_stepwise_predict<-predict.lm(lm2,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse2 <- sum((y_stepwise_predict - y_new_lm2)^2)
rsq2 <- 1 - sse2/sst
rsq2
#前进法检验R方
y_forward_predict<-predict.lm(lm3,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse3 <- sum((y_forward_predict - y_new_lm2)^2)
rsq3 <- 1 - sse3/sst
rsq3
#计算R-adjust
n<-nrow(testGroup)
k1<-15
k2<-lm2$rank-1
k3<-lm3$rank-1
radjust_lasso<-1-(1-rsq)*(n-1)/(n-k1-1)
radjust_lasso
radjust_stepwise<-1-(1-rsq2)*(n-1)/(n-k2-1)
radjust_stepwise
radjust_forward<-1-(1-rsq3)*(n-1)/(n-k3-1)
radjust_forward
#lasso回归
car_price$car_ID<-NULL
car_price_re$price<-(car_price_re$price^lambda-1)/lambda
library(glmnet)
x<-as.matrix(car_price_re[colnames(car_price_re)!="price"])
y<-as.matrix(car_price_re[colnames(car_price_re)=="price"])
fit<-glmnet(x,y,alpha=1)
#lasso筛选变量动态过程图
plot(fit,xvar="lambda",label=T)
#我们可以看到，当lambda越大，各估计参数相应的也被压缩得更小，而当lambda达到一定值以后，一部分不重要的变量将被压缩为0，代表该变量已被剔除出模型，图中从左至左右断下降的曲线如同被不断增大的lambda一步一步压缩，直到压缩为0。
#利用交叉检验挑选lambda，选择平均误差最小的lambda
model_cv<-cv.glmnet(x,y,alpha=1)
plot(model_cv)
best_lambda <- model_cv$lambda.min
best_lambda
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)
#如变量没有显示系数，即lasso回归收缩系数为零。这意味着它完全被排除在模型之外，因为它的影响力不够。系数非0的变量即为我们筛选的重要特征.
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)
#如变量没有显示系数，即lasso回归收缩系数为零。这意味着它完全被排除在模型之外，因为它的影响力不够。系数非0的变量即为我们筛选的重要特征.
#lasso回归
car_price_re$car_ID<-NULL
car_price_re$price<-(car_price_re$price^lambda-1)/lambda
library(glmnet)
x<-as.matrix(car_price_re[colnames(car_price_re)!="price"])
y<-as.matrix(car_price_re[colnames(car_price_re)=="price"])
fit<-glmnet(x,y,alpha=1)
#lasso筛选变量动态过程图
plot(fit,xvar="lambda",label=T)
#我们可以看到，当lambda越大，各估计参数相应的也被压缩得更小，而当lambda达到一定值以后，一部分不重要的变量将被压缩为0，代表该变量已被剔除出模型，图中从左至左右断下降的曲线如同被不断增大的lambda一步一步压缩，直到压缩为0。
#利用交叉检验挑选lambda，选择平均误差最小的lambda
model_cv<-cv.glmnet(x,y,alpha=1)
plot(model_cv)
best_lambda <- model_cv$lambda.min
best_lambda
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)
#如变量没有显示系数，即lasso回归收缩系数为零。这意味着它完全被排除在模型之外，因为它的影响力不够。系数非0的变量即为我们筛选的重要特征.
tidy_lasso <- broom::tidy(lasso_model)
tidy_lasso
#lasso回归检验R方
testGroup$car_ID<-NULL
x_new<-as.matrix(testGroup[colnames(testGroup)!="price"])
y_new<-as.matrix(testGroup[colnames(testGroup)=="price"])
y_new_lm2<-(y_new^lambda-1)/lambda
y_lasso_predict <- predict(lasso_model, s = best_lambda, newx = x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse <- sum((y_lasso_predict - y_new_lm2)^2)
rsq <- 1 - sse/sst
rsq
#逐步回归检验R方
x_new<-data.frame(x_new)
y_stepwise_predict<-predict.lm(lm2,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse2 <- sum((y_stepwise_predict - y_new_lm2)^2)
rsq2 <- 1 - sse2/sst
rsq2
#前进法检验R方
y_forward_predict<-predict.lm(lm3,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse3 <- sum((y_forward_predict - y_new_lm2)^2)
rsq3 <- 1 - sse3/sst
rsq3
library(nnet)
#获取数据
car_price<-read.csv("c:/users/12938/desktop/re/CarPrice_Assignment.csv",header=T)
temp<-car_price
car_price
#处理数据
car_name<-car_price[,c(1,3)]
car_price<-car_price[,-3]
a <- class.ind(car_price$fueltype)
car_fueltype<-car_price[,c("car_ID","fueltype")]
car_price<-cbind(car_price,a)
car_price$fueltype<-NULL
a <- class.ind(car_price$aspiration)
car_aspiration<-car_price[,c("car_ID","aspiration")]
car_price<-cbind(car_price,a)
car_price$aspiration<-NULL
car_price$doornumber<-ifelse(car_price$doornumber=='two',2,4)
a <- class.ind(car_price$carbody)
car_carbody<-car_price[,c("car_ID","carbody")]
car_price<-cbind(car_price,a)
car_price$carbody<-NULL
a <- class.ind(car_price$drivewheel)
car_drivewheel<-car_price[,c("car_ID","drivewheel")]
car_price<-cbind(car_price,a)
car_price$drivewheel<-NULL
a <- class.ind(car_price$enginelocation)
car_enginelocation<-car_price[,c("car_ID","enginelocation")]
car_price<-cbind(car_price,a)
car_price$enginelocation<-NULL
a <- class.ind(car_price$enginetype)
car_enginetype<-car_price[,c("price","enginetype")]
car_price<-cbind(car_price,a)
car_price$enginetype<-NULL
car_price$cylindernumber[car_price$cylindernumber=='four']<-4
car_price$cylindernumber[car_price$cylindernumber=='six']<-6
car_price$cylindernumber[car_price$cylindernumber=='five']<-5
car_price$cylindernumber[car_price$cylindernumber=='three']<-3
car_price$cylindernumber[car_price$cylindernumber=='twelve']<-12
car_price$cylindernumber[car_price$cylindernumber=='two']<-2
car_price$cylindernumber[car_price$cylindernumber=='eight']<-8
car_price$cylindernumber<-as.numeric(car_price$cylindernumber)
a <- class.ind(car_price$fuelsystem)
car_fuelsystem<-car_price[,c("price","fuelsystem")]
car_price<-cbind(car_price,a)
car_price$fuelsystem<-NULL
car_price_re<-car_price
car_price_re$car_ID<-NULL
lm1<-lm(price~.,car_price_re)
summary(lm1)
#此时进行多元线性回归，发现有数个变量有NA，这是因为在将分类变量处理成虚拟变量时，衍生的虚拟变量有多重共线性，例如，fueltype产生的两个虚拟变量diesel和gas对应元素相加为一，这种情况下，我们可以删除有多重共线性的虚拟变量，例如，drivewheel衍生的4wd，fwd，rwd三个虚拟变量，删去rwd，而4wd和fwd均为0时就是rwd。
library(MASS)
stud_resids <- studres(lm1)
plot(car_price_re$price, stud_resids, ylab=' Studentized Residuals ', xlab=' Displacement ')
car_price_renew1<-car_price_re[-17,]
lmnew1<-lm(price~.,car_price_renew1)
stud_residsnew1 <- studres(lmnew1)
plot(car_price_renew1$price, stud_residsnew1, ylab=' Studentized Residuals ', xlab=' Displacement ')
stud_residsnew1
car_price_re$gas<-NULL
car_price_re$turbo<-NULL
car_price_re$wagon<-NULL
car_price_re$rwd<-NULL
car_price_re$rear<-NULL
car_price_re$ohcv<-NULL
car_price_re$mfi<-NULL
##这两个虚拟变量出现NA的原因似乎不是因为产生虚拟变量时出现的多重共线性？##
car_price_re$rotor<-NULL
car_price_re$idi<-NULL
#抽取30%的数据作为检验组
set.seed(2021)
n<-round(nrow(car_price_re)*3/10)
uni<-sample(1:nrow (car_price_re),n,replace=T)
testGroup<-car_price_re[uni,]
car_price_re<-car_price_re[-uni,]
#lasso回归
car_price$car_ID<-NULL
library(glmnet)
x<-as.matrix(car_price_re[colnames(car_price_re)!="price"])
y<-as.matrix(car_price_re[colnames(car_price_re)=="price"])
fit<-glmnet(x,y,alpha=1)
#lasso筛选变量动态过程图
plot(fit,xvar="lambda",label=T)
#我们可以看到，当lambda越大，各估计参数相应的也被压缩得更小，而当lambda达到一定值以后，一部分不重要的变量将被压缩为0，代表该变量已被剔除出模型，图中从左至左右断下降的曲线如同被不断增大的lambda一步一步压缩，直到压缩为0。
#利用交叉检验挑选lambda，选择平均误差最小的lambda
model_cv<-cv.glmnet(x,y,alpha=1)
plot(model_cv)
best_lambda <- model_cv$lambda.min
best_lambda
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)
#如变量没有显示系数，即lasso回归收缩系数为零。这意味着它完全被排除在模型之外，因为它的影响力不够。系数非0的变量即为我们筛选的重要特征.
library(ggplot2)
x=car_price$curbweight
df<-data.frame(x=car_price$curbweight,y=car_price$price)
df1<-data.frame(x=car_price$horsepower,y=car_price$price)
df4<-data.frame(x=car_price$carlength,y=car_price$price)
ggplot(df,aes(x=x,y=y))+geom_point()+labs(title='relationship')
ggplot(df1,aes(x=x,y=y))+geom_point()+labs(title='relationship')
ggplot(df4,aes(x=x,y=y))+geom_point()+labs(title='relationship')
library(Hmisc)
df<-car_price_re[,c('curbweight','front','mpfi','carwidth','l','fwd','sedan','ohcf','horsepower','convertible','price')]
res2 <- rcorr(as.matrix(df))
res2$r
library(corrplot)
corrplot(res2$r, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
lm1<-lm(price~.,car_price_re)
lm1
plot(lm1)
#从残差图看出，误差项具有明显的异方差性
rstudent(lm1)
#|SRE|<3，说明不存在因变量的异常值
#计算库克距离
cook<-cooks.distance(lm1)
sorted_cook<-cook[order(cook,decreasing=TRUE)]
plot(sorted_cook,type='o',pch=16,xlab='observation',ylab="cook's distance")
abline(h=0.5,lty=2,col='red')
#cook distance<1，不认为有自变量异常点
library(MASS)
bc<-boxcox(price~.,data=car_price_re)
lambda <- bc$x[which.max(bc$y)]
lambda
car_price_re_bc<-car_price_re
car_price_re_bc$price<-(car_price_re$price^lambda-1)/lambda
#lasso回归
library(glmnet)
x<-as.matrix(car_price_re_bc[colnames(car_price_re_bc)!="price"])
y<-as.matrix(car_price_re_bc[colnames(car_price_re_bc)=="price"])
fit<-glmnet(x,y,alpha=1)
#lasso筛选变量动态过程图
plot(fit,xvar="lambda",label=T)
#我们可以看到，当lambda越大，各估计参数相应的也被压缩得更小，而当lambda达到一定值以后，一部分不重要的变量将被压缩为0，代表该变量已被剔除出模型，图中从左至左右断下降的曲线如同被不断增大的lambda一步一步压缩，直到压缩为0。
#利用交叉检验挑选lambda，选择平均误差最小的lambda
model_cv<-cv.glmnet(x,y,alpha=1)
plot(model_cv)
best_lambda <- model_cv$lambda.min
best_lambda
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)
#如变量没有显示系数，即lasso回归收缩系数为零。这意味着它完全被排除在模型之外，因为它的影响力不够。系数非0的变量即为我们筛选的重要特征.
tidy_lasso <- broom::tidy(lasso_model)
tidy_lasso
#lasso回归检验R方
testGroup$car_ID<-NULL
x_new<-as.matrix(testGroup[colnames(testGroup)!="price"])
y_new<-as.matrix(testGroup[colnames(testGroup)=="price"])
y_new_lm2<-(y_new^lambda-1)/lambda
y_lasso_predict <- predict(lasso_model, s = best_lambda, newx = x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse <- sum((y_lasso_predict - y_new_lm2)^2)
rsq <- 1 - sse/sst
rsq
#逐步回归检验R方
x_new<-data.frame(x_new)
y_stepwise_predict<-predict.lm(lm2,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse2 <- sum((y_stepwise_predict - y_new_lm2)^2)
rsq2 <- 1 - sse2/sst
rsq2
#前进法检验R方
y_forward_predict<-predict.lm(lm3,x_new)
sst <- sum((y_new_lm2 - mean(y_new_lm2))^2)
sse3 <- sum((y_forward_predict - y_new_lm2)^2)
rsq3 <- 1 - sse3/sst
rsq3
#计算R-adjust
n<-nrow(testGroup)
k1<-15
k2<-lm2$rank-1
k3<-lm3$rank-1
radjust_lasso<-1-(1-rsq)*(n-1)/(n-k1-1)
radjust_lasso
radjust_stepwise<-1-(1-rsq2)*(n-1)/(n-k2-1)
radjust_stepwise
radjust_forward<-1-(1-rsq3)*(n-1)/(n-k3-1)
radjust_forward
y_lasso_predict<-log(y_lasso_predict*lambda+1,lambda)
y_stepwise_predict<-log(y_stepwise_predict*lambda+1,lambda)
y_forward_predict<-log(y_forward_predict*lambda+1,lambda)
#使用预测组计算MSE判断最优模型
n_new<-nrow(testGroup)
mse_lasso<-mean((y_new_lm2-y_lasso_predict)^2)
mse_stepwise<-mean((y_new_lm2-y_stepwise_predict)^2)
mse_forward<-mean((y_new_lm2-y_forward_predict)^2)
mse_lasso
mse_stepwise
mse_forward
setwd("c://users/12938/Desktop/Rpackage")
getwd()
library(RcppArmadillo)
RcppArmadillo.package.skeleton("stepRegressionC")
setwd()
getwd()
setwd("c://users/12938/Desktop/Rpackage/stepRecessionC")
setwd("c://users/12938/Desktop/Rpackage/stepRegressionC")
setwd("c://users/12938/Desktop/Rpackage")
